머신러닝은 주로 원핫인코딩으로 결과를 나타낸다.
- 해당 형태 특성상 정보가 희박하다. 
- 0으로 공간 낭비가 심하다.
- 단어간 유사도를 알기 어렵다.

해당 문제들을 임베딩을 사용하여 해결할 수 있다.
- 차원 축소
- 단어간 유사도를 알 수 있음
- 희박한 데이터 형태를 dense하게 표현가능

원핫 인코딩의 차원은 어휘집합크기 \* 1이며
임베딩 벡터의 차원은 임베딩벡터크기 \* 어휘집합 크기로 정의할 수 있다.
두 벡터간의 곱의 결과는 임베딩 벡터크기 \* 1 이 된다.
![[스크린샷 2024-03-09 10.23.33.png]]

당연하지만 어휘집합마다 임베딩 벡터는 다를 수밖에 없다.
RAG에 활용할 때 모델마다 적합한 임베딩 벡터를 사용해야한다.

경향성이 벡터에 표현되므로 사칙연산이 가능하다.
king - man + woman =(유사) queen

