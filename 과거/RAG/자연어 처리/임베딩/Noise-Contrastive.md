![[스크린샷 2024-03-09 12.39.18.png]]
위는 CBOW모델이며 화살표 방향을 반대로 해야한다.
위 처럼 각 인덱스의 단어마다 모든 어휘집합이 라벨하나를 학습하기 위해 연산돼야하기 때문에 학습비용이 너무 크다.
따라서 이를 방지하기 위해 다음과 같은 방법으로 개선한다.

- 이진 분류 문제로 변환
- 정답 컨텍스트 1개와 정답이 아닌 무작위 컨텍스트 k개를 생성해 학습시킨다.
- https://velog.io/@finallazy/Monte-Carlo-Prediction 을 통해 근사치를 추정한다.
- 
- 방대한 어휘집합 수보다 k개가 적으므로 효율적으로 학습할 수 있다.
![[스크린샷 2024-03-09 13.03.48.png]]
Qtheta(D=1|target, history)는 데이터셋 D에서 history에서 도출된 단어가 target일 확률을 이진회귀분석한 확률입니다.

- 요점은 k개의 노이즈셋이 모든 어휘집합 개수보다는 확실하게 적다.
- 이런 방법을 Negative sampling이라고 한다.
- noise가 될 확률을 최소로하고 정답일 확률이 최대가 될 때까지 회귀분석을 반복한다.
- 학습모델이 이를 반복해 올베를 위치로 벡터를 이동시킨다.
![[스크린샷 2024-03-09 13.28.19.png]]
결과적으로 위 사진처럼 유사한 의미를 가진 벡터가 가까이 배치된다.
이는 정사영을 통해 차원수를 줄여 좀 더 명료하게 확인할 수 있다.

